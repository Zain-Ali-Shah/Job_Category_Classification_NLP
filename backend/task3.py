# -*- coding: utf-8 -*-
"""Task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YVOWUQ6EAcCM6mkTWS2Pu7jdgy_6VsPq
"""

import pandas as pd

df = pd.read_json('/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/preprocessed_ads.json')
print(df.shape)

df.head()

from google.colab import drive
drive.mount('/content/drive')

import spacy

# load english language model and create nlp object from it
nlp = spacy.load("en_core_web_sm")

def preprocess(text):
    # remove stop words and lemmatize the text
    doc = nlp(text)
    filtered_tokens = []
    for token in doc:
        if token.is_stop or token.is_punct:
            continue
        filtered_tokens.append(token.lemma_)

    return " ".join(filtered_tokens)

df.Category.value_counts()

min_samples = 156 # we have these many Sales job category and Sales is our minority class

df_Engineering = df[df.Category=="Engineering"].sample(min_samples, random_state=101)
df_Healthcare_Nursing = df[df.Category=="Healthcare_Nursing"].sample(min_samples, random_state=101)
df_Accounting_Finance = df[df.Category=="Accounting_Finance"].sample(min_samples, random_state=101)
df_Sales = df[df.Category=="Sales"].sample(min_samples, random_state=101)

df_balanced = pd.concat([df_Engineering,df_Healthcare_Nursing,df_Accounting_Finance,df_Sales],axis=0)
df_balanced.Category.value_counts()

target = {'Engineering': 0, 'Healthcare_Nursing': 1, 'Accounting_Finance': 2, 'Sales': 3}

df_balanced['Category_num'] = df_balanced['Category'].map({
    'Engineering': 0,
    'Healthcare_Nursing': 1,
    'Accounting_Finance': 2,
    'Sales': 3
})

df_balanced.head()

"""1: Model Building Using Count Vectorizer

Model Training using only description of job advertisement
"""

df_balanced['Preprocessed_Description'] = df_balanced['Description'].apply(preprocess)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Description,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

print(X_train.shape)
X_train.head()

y_train.value_counts()

y_test.value_counts()

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

"""MultinomialNB Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams or bigrams or trigrams
    'vectorizer_bow__max_df': [0.75, 1.0],           # Maximum document frequency
    'multi_nb__alpha': [0.1, 1.0, 10.0]              # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Loistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""SVM Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Gradient Boosting Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('gradient_boost', GradientBoostingClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'gradient_boost__n_estimators': [50, 100, 200],
    'gradient_boost__learning_rate': [0.01, 0.1, 0.2]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""KNN Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('knn', KNeighborsClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'knn__n_neighbors': [3, 5, 7],
    'knn__weights': ['uniform', 'distance']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Adaptive Boosting Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('adaboost', AdaBoostClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'adaboost__n_estimators': [50, 100, 200],
    'adaboost__learning_rate': [0.01, 0.1, 1]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

# !pip install tensorflow
!pip install keras

"""Deep Learning Models"""

import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, SimpleRNN, SpatialDropout1D
from keras.optimizers import Adam
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Encode the labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Maximum number of words in the vocabulary
max_words = 5200
# Maximum length of the sequences
max_len = 100

# Vectorize the text data
vectorizer = CountVectorizer(max_features=max_words)
X_train_counts = vectorizer.fit_transform(X_train).toarray()
X_test_counts = vectorizer.transform(X_test).toarray()

# Pad the sequences
X_train_padded = pad_sequences(X_train_counts, maxlen=max_len)
X_test_padded = pad_sequences(X_test_counts, maxlen=max_len)

# Function to create a simple neural network model
def create_dnn_model():
    model = Sequential()
    model.add(Dense(512, input_shape=(max_len,), activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Function to create a CNN model
def create_cnn_model():
    model = Sequential()
    model.add(Embedding(max_words, 128, input_length=max_len))
    model.add(Conv1D(128, 5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Function to create an RNN model
def create_rnn_model():
    model = Sequential()
    model.add(Embedding(max_words, 128, input_length=max_len))
    model.add(SimpleRNN(100))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

"""Artificial NN Model"""

# Train and evaluate the DNN model
dnn_model = create_dnn_model()
dnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_dnn_prob = dnn_model.predict(X_test_padded)
y_pred_dnn = np.argmax(y_pred_dnn_prob, axis=1)
print("DNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_dnn))

X_test[:5]

y_pred[:5]

"""Convolutional NN Model"""

# Train and evaluate the CNN model
cnn_model = create_cnn_model()
cnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_cnn_prob = cnn_model.predict(X_test_padded)
y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)
print("CNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_cnn))

X_test[:5]

y_pred[:5]

"""Recurrent NN Model"""

# Train and evaluate the RNN model
rnn_model = create_rnn_model()
rnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_rnn_prob = rnn_model.predict(X_test_padded)
y_pred_rnn = np.argmax(y_pred_rnn_prob, axis=1)
print("RNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_rnn))

X_test[:5]

y_pred[:5]

import spacy

# load english language model and create nlp object from it
nlp = spacy.load("en_core_web_sm")

def preprocess_title(text):
    # remove stop words and lemmatize the text
    doc = nlp(text)
    filtered_tokens = []
    for token in doc:
        if token.is_stop or token.is_punct:
            continue
        filtered_tokens.append(token.lemma_.lower())

    return " ".join(filtered_tokens)

"""Model Training using only title of job advertisement"""

df_balanced['Preprocessed_Title'] = df_balanced['Title'].apply(preprocess_title)

df_balanced.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Title,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

print(X_train.shape)
X_train.head()

y_train.value_counts()

y_test.value_counts()

"""MultinomialNB Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams or bigrams or trigrams
    'vectorizer_bow__max_df': [0.75, 1.0],           # Maximum document frequency
    'multi_nb__alpha': [0.1, 1.0, 10.0]              # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Logistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""SVM Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Model Training using both title and description of job advertisement"""

df_balanced['Preprocessed_Title_Description']=df_balanced['Preprocessed_Title']+ " " + df_balanced['Preprocessed_Description']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Title_Description,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

print(X_train.shape)
X_train.head()

y_train.value_counts()

y_test.value_counts()

"""MultiNomialNB Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams, trigrams,
    'vectorizer_bow__max_df': [0.5, 0.75, 0.85, 1.0],         # Maximum document frequency
    'vectorizer_bow__min_df': [0.01, 0.05, 0.1],              # Minimum document frequency
    'multi_nb__alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0] # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

from joblib import dump,load

best_vectorizer = grid_search.best_estimator_.named_steps['vectorizer_bow']
dump(best_vectorizer, '/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/vectorizer_count.pkl')

dump(grid_search, '/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/MultinomialNB_CountVectorizer_model.joblib')

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

from matplotlib import pyplot as plt
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Prediction')
plt.ylabel('Truth')

X_test[:5]

y_pred[:5]

"""Logistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""SVM Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_bow', CountVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_bow__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_bow__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""2 : Model Building Using using TF-IDF Vectorizer

Model Training using only the description of job advertisement
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Description,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

print(X_train.shape)
X_train.head()

y_train.value_counts()

y_test.value_counts()

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)

"""MultinomialNB Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object for TF-IDF weighted
pipeline_tfidf = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid_tfidf = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams or bigrams or trigrams
    'vectorizer_tfidf__max_df': [0.75, 1.0],                   # Maximum document frequency
    'multi_nb__alpha': [0.1, 1.0, 10.0]                        # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search_tfidf = GridSearchCV(pipeline_tfidf, param_grid_tfidf, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search_tfidf.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found for TF-IDF: ", grid_search_tfidf.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred_tfidf = grid_search_tfidf.predict(X_test)

# Print the classification report
print("TF-IDF Weighted Classification Report:")
print(classification_report(y_test, y_pred_tfidf))

X_test[:5]

y_pred[:5]

"""Logistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""SVM Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""KNN Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('knn', KNeighborsClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'knn__n_neighbors': [3, 4, 5, 6, 7, 8],
    'knn__weights': ['uniform', 'distance']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Gradient Boosting Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('gradient_boost', GradientBoostingClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'gradient_boost__n_estimators': [50, 100, 200],
    'gradient_boost__learning_rate': [0.01, 0.1, 0.2]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[0:5]

"""Adaptive Boosting Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('adaboost', AdaBoostClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'adaboost__n_estimators': [50, 100, 200],
    'adaboost__learning_rate': [0.01, 0.1, 1]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

X_test[:5]

y_pred[:5]

"""Artificial NN Model"""

# Train and evaluate the DNN model
dnn_model = create_dnn_model()
dnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_dnn_prob = dnn_model.predict(X_test_padded)
y_pred_dnn = np.argmax(y_pred_dnn_prob, axis=1)
print("DNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_dnn))

y_pred[:5]

"""Convolutional NN Model"""

# Train and evaluate the CNN model
cnn_model = create_cnn_model()
cnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_cnn_prob = cnn_model.predict(X_test_padded)
y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)
print("CNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_cnn))

y_pred[:5]

"""Recurrent NN Model"""

# Train and evaluate the RNN model
rnn_model = create_rnn_model()
rnn_model.fit(X_train_padded, y_train_encoded, epochs=5, batch_size=32, verbose=1)
y_pred_rnn_prob = rnn_model.predict(X_test_padded)
y_pred_rnn = np.argmax(y_pred_rnn_prob, axis=1)
print("RNN Model Classification Report:")
print(classification_report(y_test_encoded, y_pred_rnn))

y_pred[:5]

"""Model Training using only the title  of job advertisement"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Title,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

"""MultinomialNB Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object for TF-IDF weighted
pipeline_tfidf = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid_tfidf = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams or bigrams or trigrams
    'vectorizer_tfidf__max_df': [0.75, 1.0],                   # Maximum document frequency
    'multi_nb__alpha': [0.1, 1.0, 10.0]                        # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search_tfidf = GridSearchCV(pipeline_tfidf, param_grid_tfidf, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search_tfidf.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found for TF-IDF: ", grid_search_tfidf.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred_tfidf = grid_search_tfidf.predict(X_test)

# Print the classification report
print("TF-IDF Weighted Classification Report:")
print(classification_report(y_test, y_pred_tfidf))

y_pred[:5]

"""Loistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""SVM Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""KNN Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('knn', KNeighborsClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'knn__n_neighbors': [3, 4, 5, 6, 7, 8],
    'knn__weights': ['uniform', 'distance']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""Model Training using both title and description of job advertisement"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_balanced.Preprocessed_Title_Description,
    df_balanced.Category_num,
    test_size=0.2, # 20% samples will go to test dataset
    random_state=101,
    stratify=df_balanced.Category_num
)

"""MultinomialNB Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object for TF-IDF weighted
pipeline_tfidf = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('multi_nb', MultinomialNB())
])

# Define the parameter grid for hyperparameter tuning
param_grid_tfidf = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams or bigrams or trigrams
    'vectorizer_tfidf__max_df': [0.75, 1.0],                   # Maximum document frequency
    'multi_nb__alpha': [0.1, 1.0, 10.0]                        # Smoothing parameter
}

# Initialize GridSearchCV with cross-validation
grid_search_tfidf = GridSearchCV(pipeline_tfidf, param_grid_tfidf, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search_tfidf.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found for TF-IDF: ", grid_search_tfidf.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred_tfidf = grid_search_tfidf.predict(X_test)

# Print the classification report
print("TF-IDF Weighted Classification Report:")
print(classification_report(y_test, y_pred_tfidf))

y_pred[:5]

"""Logistic Regression Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('log_reg', LogisticRegression(max_iter=1000))
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'log_reg__C': [0.01, 0.1, 1, 10]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""SVM Model"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('svc', SVC())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams, trigrams
    'vectorizer_tfidf__max_df': [0.5, 0.75, 1.0],              # Maximum document frequency
    'svc__C': [0.01, 0.1, 1, 10,],                       # Regularization parameter
    'svc__kernel': ['linear', 'rbf'],     # Kernel type
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

from joblib import dump,load

best_vectorizer = grid_search.best_estimator_.named_steps['vectorizer_tfidf']
dump(best_vectorizer, '/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/vectorizer_tfidf_count.pkl')

dump(grid_search, '/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/MultinomialNB_CountVectorizer_model.joblib')

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

from matplotlib import pyplot as plt
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Prediction')
plt.ylabel('Truth')

y_pred[:5]

"""Random Forest Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('random_forest', RandomForestClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0, 2],
    'random_forest__n_estimators': [50, 200, 500],
    'random_forest__max_depth': [None, 10, 20]
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""KNN Model"""

# Define a pipeline object
pipeline = Pipeline([
     ('vectorizer_tfidf', TfidfVectorizer()),
     ('knn', KNeighborsClassifier())
])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'vectorizer_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
    'vectorizer_tfidf__max_df': [0.75, 1.0],
    'knn__n_neighbors': [3, 4, 5, 6, 7, 8],
    'knn__weights': ['uniform', 'distance']
}

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

# Fit the model with X_train and y_train
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Get the predictions for X_test and store it in y_pred
y_pred = grid_search.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

y_pred[:5]

"""Model Testing on unseen data"""

import pandas as pd

# Example job posts
data = [
    {
        "Category": "Engineering",
        "Title": "Mechanical Design Engineer",
        "Webindex": "72635560",
        "Company": "Innovative Engineering Solutions",
        "Description": "mechanical design engineer responsible developing innovative mechanical systems solutions collaborate cross functional teams ensure product quality efficiency extensive knowledge cad software solidworks autocad experience mechanical design analysis simulation stress testing required minimum bachelor's degree mechanical engineering strong problem solving communication skills competitive salary benefits package apply send resume company email visit website"
    },
    {
        "Category": "Healthcare_Nursing",
        "Title": "Registered Nurse - Intensive Care Unit",
        "Webindex": "72635561",
        "Company": "Healthcare Professionals",
        "Description": "registered nurse icu intensive care unit critical patient monitoring vital signs administering medications patient care plans healthcare professionals team environment shift rotations extensive benefits experience icu nursing registered nurse license state requirements minimum bachelor nursing bsn apply position send cv email website contact online healthcare professionals url removed"
    },
    {
        "Category": "Accounting_Finance",
        "Title": "Senior Financial Analyst",
        "Webindex": "72635562",
        "Company": "Global Financial Services",
        "Description": "senior financial analyst budgeting forecasting financial planning reporting financial statements variance analysis accounting gaap regulations financial modeling investment analysis strategic planning teamwork leadership skills excel sap minimum cpa mba preferred financial services global company structured benefits package send resume apply position online visit website url removed"
    },
    {
        "Category": "Sales",
        "Title": "Sales Representative - Regional Accounts",
        "Webindex": "72635563",
        "Company": "National Sales Corp",
        "Description": "sales representative regional accounts client relationship management territory sales target achievement lead generation product demonstrations customer feedback market analysis sales strategy teamwork communication skills minimum bachelor business administration bba proven sales track record regional sales national sales corp career opportunity benefits package apply send resume visit website url removed"
    }
]

# Create DataFrame
df_examples = pd.DataFrame(data)

# Display DataFrame
df_examples

df_examples['Preprocessed_Description'] = df_examples['Description'].apply(preprocess)

df_examples['Preprocessed_Title'] = df_examples['Title'].apply(preprocess_title)

df_examples['Preprocessed_Title_Description']=df_examples['Preprocessed_Title']+ " " + df_examples['Preprocessed_Description']

df_examples

from joblib import load

model1 = load('/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/MultinomialNB_CountVectorizer_model.joblib')

y_pred_examples = model1.predict(df_examples['Preprocessed_Title_Description'])

y_pred_examples

model2 = load('/content/drive/MyDrive/Natural Language Processing/Assignment5_Solution/SVC_tfidf_model.joblib')

y_pred_examples = model2.predict(df_examples['Preprocessed_Title_Description'])

y_pred_examples

